import os, time
os.environ["CUDA_VISIBLE_DEVICES"]="-1" 
import pprint
import codecs
import array
import collections
import io
import numpy as np
try:
    import cPickle as pickle # for Python 2
except ImportError:
    import pickle # for Python 3
import tensorflow as tf
from dataset import *
from hlstm import TextLSTM

pp = pprint.PrettyPrinter()
flags = tf.app.flags
flags.DEFINE_integer("batch_size", 32, "notice...")
flags.DEFINE_integer("iterations", 100, "notice...")
flags.DEFINE_float("learning_lr", 0.01, "notice...")
flags.DEFINE_boolean("reload_word_emb", False, "reload wordembedding from GloVe or use saved one [False]")
flags.DEFINE_string("word_emb_path", "../glove.6B/glove.6B.50d.txt", "notice...")
flags.DEFINE_integer("word_emb_dim", 50, "notice...")
flags.DEFINE_integer("emb_dim", 50, "notice...")
flags.DEFINE_string("train_data_path", "../train.csv", "notice...")
flags.DEFINE_string("test_data_path", "../test.csv", "notice...")
flags.DEFINE_string("checkpoint_dir", "../checkpoints", "checkpoint directory [checkpoints]")
flags.DEFINE_integer("class_cnt", 2, "notice...")
flags.DEFINE_boolean("debug", False, "[False]")
flags.DEFINE_boolean("show", True, "[True]")
FLAGS = flags.FLAGS

def load_stanford(filename):
    """
    Load model from the output files generated by
    the C code from http://nlp.stanford.edu/projects/glove/.
    The entries of the word dictionary will be of type
    unicode in Python 2 and str in Python 3.
    """

    dct = {}
    vectors = array.array('d')

    # Read in the data.
    # with codecs.open(filename, 'r', encoding='ISO-8859-1') as savefile:
        # for i, line in enumerate(savefile):
    with open(filename, 'r') as savefile:
        i = 0
        for line in savefile:
            tokens = line.split(' ')

            word = tokens[0]
            entries = tokens[1:]

            dct[word] = i
            vectors.extend(float(x) for x in entries)
            i += 1
            # Infer word vectors dimensions.

        no_components = len(entries)
        no_vectors = len(dct)
        print("Corpus stats: ", no_components, no_vectors)

        # Make these into numpy arrays
        word_vecs = np.array(vectors).reshape(no_vectors, no_components)
        # print(word_vecs.shape)
        # print(word_vecs[399999])
        inverse_dictionary = {v: k for k, v in dct.items()}
        # print(inverse_dictionary[399999])
        return (word_vecs, dct, inverse_dictionary)

# def get_word_emb(voc, word):
#     try:
#         return voc[word]
#     except:
#         return -1

def main(_):
    pp.pprint(flags.FLAGS.__flags)
    # LOAD WORD VECTORS and VOC
    if FLAGS.reload_word_emb == True:
        wordVectors, voc, voc_inv = load_stanford(FLAGS.word_emb_path)
        f = open('../wordVectors.save', 'wb')
        pickle.dump(wordVectors, f)
        f.close()
        f = open('../voc.save', 'wb')
        pickle.dump(voc, f)
        f.close()
        f = open('../voc_inv.save', 'wb')
        pickle.dump(voc_inv, f)
        f.close()
    else:
        f = open('../wordVectors.save', 'rb')
        wordVectors = pickle.load(f, encoding='latin1')
        f = open('../voc.save', 'rb')
        voc = pickle.load(f, encoding='latin1')
        f = open('../voc_inv.save', 'rb')
        voc_inv = pickle.load(f, encoding='latin1')

    print('-> wordVectors dim: ', np.shape(wordVectors))
    print('-> voc size: ', len(voc))
    # print(wordVectors[3998])
    # print(voc['brings'])

    # LOAD DATA
    train_data = Dataset(FLAGS.train_data_path, voc, FLAGS.batch_size, FLAGS.word_emb_dim)
    # print(train_data.docs[0][0])
    test_data = Dataset(FLAGS.test_data_path, voc, FLAGS.batch_size, FLAGS.word_emb_dim, prev_wordinsent_cnt = train_data.wordinsent_cnt)
    # print(test_data.docs[0][0])
    wordinsent_cnt = max(train_data.wordinsent_cnt, test_data.wordinsent_cnt)

    tf.reset_default_graph()
    # print ('.................................. training network')
    with tf.Session() as sess:
        model = TextLSTM(FLAGS, sess, 25, wordinsent_cnt, FLAGS.class_cnt, len(voc), FLAGS.emb_dim, FLAGS.emb_dim, wordVectors)
        model.run(train_data, test_data)


if __name__ == "__main__":
    # main()
    tf.app.run()