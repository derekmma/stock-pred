import os, time
os.environ["CUDA_VISIBLE_DEVICES"]="-1" 
import pprint
import codecs
import array
import collections
import io
import pandas
import numpy as np
import re
import string
try:
    # Python 2 compat
    import cPickle as pickle
except ImportError:
    import pickle
import tensorflow as tf
from hlstm import TextLSTM

flags = tf.app.flags
flags.DEFINE_boolean("reload_word_emb", False, "reload wordembedding from GloVe or use saved one [False]")
flags.DEFINE_string("word_emb_path", "../glove.6B/glove.6B.50d.txt", "notice...")
flags.DEFINE_string("data_path", "../data.csv", "notice...")
FLAGS = flags.FLAGS

def load_stanford(filename):
    """
    Load model from the output files generated by
    the C code from http://nlp.stanford.edu/projects/glove/.
    The entries of the word dictionary will be of type
    unicode in Python 2 and str in Python 3.
    """

    dct = {}
    vectors = array.array('d')

    # Read in the data.
    # with codecs.open(filename, 'r', encoding='ISO-8859-1') as savefile:
        # for i, line in enumerate(savefile):
    with open(filename, 'r') as savefile:
        i = 0
        for line in savefile:
            tokens = line.split(' ')

            word = tokens[0]
            entries = tokens[1:]

            dct[word] = i
            vectors.extend(float(x) for x in entries)
            i += 1
            # Infer word vectors dimensions.

        no_components = len(entries)
        no_vectors = len(dct)
        print("Corpus stats: ", no_components, no_vectors)

        # Make these into numpy arrays
        word_vecs = np.array(vectors).reshape(no_vectors, no_components)
        # print(word_vecs.shape)
        # print(word_vecs[399999])
        inverse_dictionary = {v: k for k, v in dct.items()}
        # print(inverse_dictionary[399999])
        return (word_vecs, dct, inverse_dictionary)

def main():
    # LOAD WORD VECTORS and VOC
    if FLAGS.reload_word_emb == True:
        wordVectors, voc, _ = load_stanford(FLAGS.word_emb_path)
        f = open('../wordVectors.save', 'wb')
        pickle.dump(wordVectors, f)
        f.close()
        f = open('../voc.save', 'wb')
        pickle.dump(voc, f)
        f.close()
    else:
        f = open('../wordVectors.save', 'rb')
        wordVectors = pickle.load(f, encoding='latin1')
        f = open('../voc.save', 'rb')
        voc = pickle.load(f, encoding='latin1')

    print('-> wordVectors dim: ', np.shape(wordVectors))
    print('-> voc size: ', len(voc))
    # print(wordVectors[3998])
    # print(voc['brings'])

    # LOAD DATA
    data = pandas.read_csv(FLAGS.data_path)
    training_set = data[data['Date'] <= '2014-12-31']
    testing_set = data[data['Date'] >= '2015-01-02']

    training_date = training_set['Date']
    training_label = training_set['Label']
    training_news_group = training_set.iloc[:, 2:27]
    testing_date = testing_set['Date']
    testing_label = testing_set['Label']
    testing_news_group = testing_set.iloc[:, 2:27]

    remove = string.punctuation
    remove = remove.replace("-", "") # don't remove hyphens
    pattern = r"[{}]".format(remove) # create the pattern

    # Preprocess the news by using regex
    for title in training_news_group:
        for index in range(len(training_news_group[title])):
            news = re.sub(r"(^b)([\"\'])", "", str(training_news_group[title][index]))
            news = re.sub(r"([\"\']$)", "", str(news))
            news = re.sub(pattern, "", str(news)) 
            training_news_group[title][index] = news

    print(training_news_group.head(5))

if __name__ == "__main__":
    main()